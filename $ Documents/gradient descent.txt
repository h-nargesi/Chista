[cross-entropy loss]
Error = ∑ (yi * log(ŷi) + (1-yi) * log(1-ŷi))
∂Error / ∂ŷ = y - ŷ

[errorest]
Error = 1/M ∑M ½ * (yi - ŷi)²
∂Error / ∂ŷ = 1/M ∑M (yi - ŷi)

[soft-max]
f(h) = e^h / ∑ e^hi
∂ ƒ(hi) / ∂ ha = 
if i = a => ∂ ƒ(hi) / ∂ ha = f(ha)(1 - f(ha))
if i != a => ∂ ƒ(hi) / ∂ ha = -f(hi)f(ha)

[weights change]
Δwi = -(∂Error / ∂wi)

[output layer]
e = (y - ŷ)
δ = e * ƒ’(h)
Δwi = η * δ * xi

[hidden layer]
(j=hidden layer, k=output layer)
ej = (∑k δk * wjk)
δj = ej * ƒ’(hj)
Δwi = η * δ * xi